{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Space X  Falcon 9 First Stage Landing Prediction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Web scraping Falcon 9 and Falcon Heavy Launches Records from Wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **40** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will be performing web scraping to collect Falcon 9 historical launch records from a Wikipedia page titled `List of Falcon 9 and Falcon Heavy launches`\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_Falcon_9_and_Falcon_Heavy_launches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/labs/module_1_L2/images/Falcon9_rocket_family.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Falcon 9 first stage will land successfully\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/api/Images/landing_1.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several examples of an unsuccessful landing are shown here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DS0701EN-SkillsNetwork/api/Images/crash.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically, the launch records are stored in a HTML table shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/labs/module_1_L2/images/falcon9-launches-wiki.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Objectives\n",
    "Web scrap Falcon 9 launch records with `BeautifulSoup`: \n",
    "- Extract a Falcon 9 launch records HTML table from Wikipedia\n",
    "- Parse the table and convert it into a Pandas data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's import required packages for this lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from requests) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "!pip3 install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we will provide some helper functions for you to process web scraped HTML table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def date_time(table_cells):\n",
    "    \"\"\"\n",
    "    This function returns the data and time from the HTML  table cell\n",
    "    Input: the  element of a table data cell extracts extra row\n",
    "    \"\"\"\n",
    "    return [data_time.strip() for data_time in list(table_cells.strings)][0:2]\n",
    "\n",
    "def booster_version(table_cells):\n",
    "    \"\"\"\n",
    "    This function returns the booster version from the HTML  table cell \n",
    "    Input: the  element of a table data cell extracts extra row\n",
    "    \"\"\"\n",
    "    out=''.join([booster_version for i,booster_version in enumerate( table_cells.strings) if i%2==0][0:-1])\n",
    "    return out\n",
    "\n",
    "def landing_status(table_cells):\n",
    "    \"\"\"\n",
    "    This function returns the landing status from the HTML table cell \n",
    "    Input: the  element of a table data cell extracts extra row\n",
    "    \"\"\"\n",
    "    out=[i for i in table_cells.strings][0]\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_mass(table_cells):\n",
    "    mass=unicodedata.normalize(\"NFKD\", table_cells.text).strip()\n",
    "    if mass:\n",
    "        mass.find(\"kg\")\n",
    "        new_mass=mass[0:mass.find(\"kg\")+2]\n",
    "    else:\n",
    "        new_mass=0\n",
    "    return new_mass\n",
    "\n",
    "\n",
    "def extract_column_from_header(row):\n",
    "    \"\"\"\n",
    "    This function returns the landing status from the HTML table cell \n",
    "    Input: the  element of a table data cell extracts extra row\n",
    "    \"\"\"\n",
    "    if (row.br):\n",
    "        row.br.extract()\n",
    "    if row.a:\n",
    "        row.a.extract()\n",
    "    if row.sup:\n",
    "        row.sup.extract()\n",
    "        \n",
    "    colunm_name = ' '.join(row.contents)\n",
    "    \n",
    "    # Filter the digit and empty names\n",
    "    if not(colunm_name.strip().isdigit()):\n",
    "        colunm_name = colunm_name.strip()\n",
    "        return colunm_name    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the lab tasks consistent, you will be asked to scrape the data from a snapshot of the  `List of Falcon 9 and Falcon Heavy launches` Wikipage updated on\n",
    "`9th June 2021`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, request the HTML page from the above URL and get a `response` object\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 1: Request the Falcon9 Launch Wiki page from its URL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's perform an HTTP GET method to request the Falcon9 Launch HTML page, as an HTTP response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Falcon 9 Launch Wiki page URL\n",
    "url = \"https://en.wikipedia.org/wiki/Falcon_9\"\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status code to ensure the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    html_content = response.text  # This will store the HTML content of the page\n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "\n",
    "# Output the first 500 characters of the HTML (for preview)\n",
    "print(html_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `BeautifulSoup` object from the HTML `response`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "Page Title: List of Falcon 9 and Falcon Heavy launches - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Preview the parsed data by printing the page title\n",
    "    print(f\"Page Title: {soup.title.string}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the page title to verify if the `BeautifulSoup` object was created properly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "Page Title: List of Falcon 9 and Falcon Heavy launches - Wikipedia\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Print the page title to verify the BeautifulSoup object was created properly\n",
    "    print(f\"Page Title: {soup.title.string}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 2: Extract all column/variable names from the HTML table header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to collect all relevant column names from the HTML table header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find all tables on the wiki page first. If you need to refresh your memory about `BeautifulSoup`, please check the external reference link towards the end of this lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "First table HTML structure:\n",
      "<table class=\"col-begin\" role=\"presentation\">\n",
      " <tbody>\n",
      "  <tr>\n",
      "   <td class=\"col-break\">\n",
      "    <div class=\"mw-heading mw-heading3\">\n",
      "     <h3 id=\"Rocket_configurations\">\n",
      "      Rocket configurations\n",
      "     </h3>\n",
      "    </div>\n",
      "    <div class=\"chart noresize\" style=\"padding-top:10px;margin-top:1em;max-width:420px;\">\n",
      "     <div style=\"position:relative;min-height:320px;min-width:420px;max-width:420px;\">\n",
      "      <div style=\"float:right;position:relative;min-height:240px;min-width:320px;max-width:320px;border-left:1px black solid;border-bottom:1px black solid;\">\n",
      "       <div style=\"position:absolute;left:3px;top:224px;height:15px;min-width:18px;max-width:18px;background-color:LightSteelBlue;-webkit-print-color-adjust:exact;border:1px solid LightSteelBlue;border-bottom:none;overflow:hidden;\" title=\"[[Falcon 9 v1.0]]: 2\">\n",
      "       </div>\n",
      "       <div style=\"position:absolute;left:55px;top:224px;height:15px;min-width:18px;max-width:18px;background-color:LightSteelBlue;-webkit-print-color-adjust:exact;border:1p\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # Check if any tables were found\n",
    "    if len(tables) > 0:\n",
    "        # Extract the first table\n",
    "        first_table = tables[0]\n",
    "        \n",
    "        # Print the first table's HTML structure to inspect\n",
    "        print(\"First table HTML structure:\")\n",
    "        print(first_table.prettify()[:1000])  # Print the first 1000 characters to avoid excessive output\n",
    "    else:\n",
    "        print(\"No tables found on the page.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "Column names found in the first table:\n",
      "[\"Rocket configurations\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n\\n\\n'10\\n\\n'11\\n\\n'12\\n\\n'13\\n\\n'14\\n\\n'15\\n\\n'16\\n\\n'17\\n\\n'18\\n\\n'19\\n\\n'20\\n\\n'21\\n\\n\\n\\n\\n\\xa0 Falcon 9 v1.0\\n\\xa0 Falcon 9 v1.1\\n\\xa0 Falcon 9 Full Thrust\\n\\xa0 Falcon 9 FT (reused)\\n\\xa0 Falcon 9 Block 5\\n\\xa0 Falcon 9 B5 (reused)\\n\\xa0 Falcon Heavy\", \"Launch sites\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\n\\n10\\n\\n15\\n\\n20\\n\\n25\\n\\n30\\n\\n\\n\\n'10\\n\\n'11\\n\\n'12\\n\\n'13\\n\\n'14\\n\\n'15\\n\\n'16\\n\\n'17\\n\\n'18\\n\\n'19\\n\\n'20\\n\\n'21\\n\\n\\n\\n\\n\\xa0 CCSFS, SLC-40\\n\\xa0 KSC, LC-39A\\n\\xa0 VAFB, SLC-4E\"]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # Check if any tables were found\n",
    "    if len(tables) > 0:\n",
    "        # Extract the first table\n",
    "        first_table = tables[0]\n",
    "        \n",
    "        # Find the first row (tr), which might contain column names\n",
    "        first_row = first_table.find('tr')\n",
    "        \n",
    "        # Extract and print the text content of each cell in the first row\n",
    "        column_names = [cell.text.strip() for cell in first_row.find_all(['th', 'td'])]\n",
    "        print(\"Column names found in the first table:\")\n",
    "        print(column_names)\n",
    "    else:\n",
    "        print(\"No tables found on the page.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the third table is our target table contains the actual launch records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "Third table HTML structure:\n",
      "<table class=\"wikitable plainrowheaders collapsible\" style=\"width: 100%;\">\n",
      " <tbody>\n",
      "  <tr>\n",
      "   <th scope=\"col\">\n",
      "    Flight No.\n",
      "   </th>\n",
      "   <th scope=\"col\">\n",
      "    Date and\n",
      "    <br/>\n",
      "    time (\n",
      "    <a href=\"/wiki/Coordinated_Universal_Time\" title=\"Coordinated Universal Time\">\n",
      "     UTC\n",
      "    </a>\n",
      "    )\n",
      "   </th>\n",
      "   <th scope=\"col\">\n",
      "    <a href=\"/wiki/List_of_Falcon_9_first-stage_boosters\" title=\"List of Falcon 9 first-stage boosters\">\n",
      "     Version,\n",
      "     <br/>\n",
      "     Booster\n",
      "    </a>\n",
      "    <sup class=\"reference\" id=\"cite_ref-booster_11-0\">\n",
      "     <a href=\"#cite_note-booster-11\">\n",
      "      <span class=\"cite-bracket\">\n",
      "       [\n",
      "      </span>\n",
      "      b\n",
      "      <span class=\"cite-bracket\">\n",
      "       ]\n",
      "      </span>\n",
      "     </a>\n",
      "    </sup>\n",
      "   </th>\n",
      "   <th scope=\"col\">\n",
      "    Launch site\n",
      "   </th>\n",
      "   <th scope=\"col\">\n",
      "    Payload\n",
      "    <sup class=\"reference\" id=\"cite_ref-Dragon_12-0\">\n",
      "     <a href=\"#cite_note-Dragon-12\">\n",
      "      <span class=\"cite-bracket\">\n",
      "       [\n",
      "      </span>\n",
      "      c\n",
      "      <span class=\"cite-bracket\">\n",
      "       ]\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # Ensure there are at least three tables\n",
    "    if len(tables) >= 3:\n",
    "        # Extract the third table (index starts at 0)\n",
    "        third_table = tables[2]\n",
    "        \n",
    "        # Print the third table's HTML structure for inspection\n",
    "        print(\"Third table HTML structure:\")\n",
    "        print(third_table.prettify()[:1000])  # Print the first 1000 characters for a preview\n",
    "    else:\n",
    "        print(\"Less than 3 tables found on the page.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should able to see the columns names embedded in the table header elements `<th>` as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<tr>\n",
    "<th scope=\"col\">Flight No.\n",
    "</th>\n",
    "<th scope=\"col\">Date and<br/>time (<a href=\"/wiki/Coordinated_Universal_Time\" title=\"Coordinated Universal Time\">UTC</a>)\n",
    "</th>\n",
    "<th scope=\"col\"><a href=\"/wiki/List_of_Falcon_9_first-stage_boosters\" title=\"List of Falcon 9 first-stage boosters\">Version,<br/>Booster</a> <sup class=\"reference\" id=\"cite_ref-booster_11-0\"><a href=\"#cite_note-booster-11\">[b]</a></sup>\n",
    "</th>\n",
    "<th scope=\"col\">Launch site\n",
    "</th>\n",
    "<th scope=\"col\">Payload<sup class=\"reference\" id=\"cite_ref-Dragon_12-0\"><a href=\"#cite_note-Dragon-12\">[c]</a></sup>\n",
    "</th>\n",
    "<th scope=\"col\">Payload mass\n",
    "</th>\n",
    "<th scope=\"col\">Orbit\n",
    "</th>\n",
    "<th scope=\"col\">Customer\n",
    "</th>\n",
    "<th scope=\"col\">Launch<br/>outcome\n",
    "</th>\n",
    "<th scope=\"col\"><a href=\"/wiki/Falcon_9_first-stage_landing_tests\" title=\"Falcon 9 first-stage landing tests\">Booster<br/>landing</a>\n",
    "</th></tr>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we just need to iterate through the `<th>` elements and apply the provided `extract_column_from_header()` to extract column name one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_column_from_header(header_element):\n",
    "    # Extracts the text from the header element, removing extra whitespace\n",
    "    return header_element.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "Column names found in the third table:\n",
      "['Flight No.', 'Date andtime (UTC)', 'Version,Booster [b]', 'Launch site', 'Payload[c]', 'Payload mass', 'Orbit', 'Customer', 'Launchoutcome', 'Boosterlanding', '1', '2', '3', '4', '5', '6', '7']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Function to extract column names from <th> elements\n",
    "def extract_column_from_header(header_element):\n",
    "    # Extract the column name text and strip any extra spaces\n",
    "    return header_element.text.strip()\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # Ensure there are at least three tables\n",
    "    if len(tables) >= 3:\n",
    "        # Extract the third table (index 2)\n",
    "        third_table = tables[2]\n",
    "        \n",
    "        # Find all <th> elements in the third table (table headers)\n",
    "        headers = third_table.find_all('th')\n",
    "        \n",
    "        # Extract column names from each header element\n",
    "        column_names = [extract_column_from_header(header) for header in headers]\n",
    "        \n",
    "        # Print the extracted column names\n",
    "        print(\"Column names found in the third table:\")\n",
    "        print(column_names)\n",
    "    else:\n",
    "        print(\"Less than 3 tables found on the page.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the extracted column names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "Column names found in the third table:\n",
      "['Flight No.', 'Date andtime (UTC)', 'Version,Booster [b]', 'Launch site', 'Payload[c]', 'Payload mass', 'Orbit', 'Customer', 'Launchoutcome', 'Boosterlanding', '1', '2', '3', '4', '5', '6', '7']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Function to extract column names from <th> elements\n",
    "def extract_column_from_header(header_element):\n",
    "    # Extract the column name text and strip any extra spaces\n",
    "    return header_element.text.strip()\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # Ensure there are at least three tables\n",
    "    if len(tables) >= 3:\n",
    "        # Extract the third table (index 2)\n",
    "        third_table = tables[2]\n",
    "        \n",
    "        # Find all <th> elements in the third table (table headers)\n",
    "        headers = third_table.find_all('th')\n",
    "        \n",
    "        # Extract column names from each header element\n",
    "        column_names = [extract_column_from_header(header) for header in headers]\n",
    "        \n",
    "        # Print the extracted column names\n",
    "        print(\"Column names found in the third table:\")\n",
    "        print(column_names)\n",
    "    else:\n",
    "        print(\"Less than 3 tables found on the page.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Create a data frame by parsing the launch HTML tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create an empty dictionary with keys from the extracted column names in the previous task. Later, this dictionary will be converted into a Pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "Column names found in the third table:\n",
      "['Flight No.', 'Date andtime (UTC)', 'Version,Booster [b]', 'Launch site', 'Payload[c]', 'Payload mass', 'Orbit', 'Customer', 'Launchoutcome', 'Boosterlanding', '1', '2', '3', '4', '5', '6', '7']\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 5 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Data extracted into the DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [Flight No., Date andtime (UTC), Version,Booster [b], Launch site, Payload[c], Payload mass, Orbit, Customer, Launchoutcome, Boosterlanding, 1, 2, 3, 4, 5, 6, 7]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Function to extract column names from <th> elements\n",
    "def extract_column_from_header(header_element):\n",
    "    # Extract the column name text and strip any extra spaces\n",
    "    return header_element.text.strip()\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # Ensure there are at least three tables\n",
    "    if len(tables) >= 3:\n",
    "        # Extract the third table (index 2)\n",
    "        third_table = tables[2]\n",
    "        \n",
    "        # Find all <th> elements in the third table (table headers)\n",
    "        headers = third_table.find_all('th')\n",
    "        \n",
    "        # Extract column names from each header element\n",
    "        column_names = [extract_column_from_header(header) for header in headers]\n",
    "        print(\"Column names found in the third table:\")\n",
    "        print(column_names)\n",
    "        \n",
    "        # Create an empty dictionary to hold the table data\n",
    "        table_data = {col: [] for col in column_names}\n",
    "        \n",
    "        # Find all rows in the table (skip the first one as it contains headers)\n",
    "        rows = third_table.find_all('tr')[1:]  # Skipping the header row\n",
    "        \n",
    "        # Loop through each row and extract cell data\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            \n",
    "            # Check if the number of cells matches the number of headers\n",
    "            if len(cells) == len(column_names):\n",
    "                for i, cell in enumerate(cells):\n",
    "                    table_data[column_names[i]].append(cell.text.strip())\n",
    "            else:\n",
    "                print(f\"Skipping row due to mismatch: {len(cells)} cells found, expected {len(column_names)}\")\n",
    "        \n",
    "        # Convert the dictionary into a Pandas DataFrame\n",
    "        df = pd.DataFrame(table_data)\n",
    "        \n",
    "        # Display the DataFrame\n",
    "        print(\"Data extracted into the DataFrame:\")\n",
    "        print(df.head())  # Display the first 5 rows of the DataFrame\n",
    "    else:\n",
    "        print(\"Less than 3 tables found on the page.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we just need to fill up the `launch_dict` with launch records extracted from table rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, HTML tables in Wiki pages are likely to contain unexpected annotations and other types of noises, such as reference links `B0004.1[8]`, missing values `N/A [e]`, inconsistent formatting, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the parsing process, we have provided an incomplete code snippet below to help you to fill up the `launch_dict`. Please complete the following code snippet with TODOs or you can choose to write your own logic to parse all launch tables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "Column names found in the third table:\n",
      "['Flight No.', 'Date andtime (UTC)', 'Version,Booster [b]', 'Launch site', 'Payload[c]', 'Payload mass', 'Orbit', 'Customer', 'Launchoutcome', 'Boosterlanding', '1', '2', '3', '4', '5', '6', '7']\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 5 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Data extracted into the DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [Flight No., Date andtime (UTC), Version,Booster [b], Launch site, Payload[c], Payload mass, Orbit, Customer, Launchoutcome, Boosterlanding, 1, 2, 3, 4, 5, 6, 7]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Function to extract column names from <th> elements\n",
    "def extract_column_from_header(header_element):\n",
    "    # Extract the column name text and strip any extra spaces\n",
    "    return header_element.text.strip()\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # Ensure there are at least three tables\n",
    "    if len(tables) >= 3:\n",
    "        # Extract the third table (index 2)\n",
    "        third_table = tables[2]\n",
    "        \n",
    "        # Find all <th> elements in the third table (table headers)\n",
    "        headers = third_table.find_all('th')\n",
    "        \n",
    "        # Extract column names from each header element\n",
    "        column_names = [extract_column_from_header(header) for header in headers]\n",
    "        print(\"Column names found in the third table:\")\n",
    "        print(column_names)\n",
    "        \n",
    "        # Initialize the launch_dict with empty lists for each column\n",
    "        launch_dict = {col: [] for col in column_names}\n",
    "        \n",
    "        # Find all rows in the table (skip the first one as it contains headers)\n",
    "        rows = third_table.find_all('tr')[1:]  # Skipping the header row\n",
    "        \n",
    "        # Loop through each row and extract cell data\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            \n",
    "            # Check if the number of cells matches the number of headers\n",
    "            if len(cells) == len(column_names):\n",
    "                for i, cell in enumerate(cells):\n",
    "                    launch_dict[column_names[i]].append(cell.text.strip())  # Append cell data to the corresponding column\n",
    "            else:\n",
    "                print(f\"Skipping row due to mismatch: {len(cells)} cells found, expected {len(column_names)}\")\n",
    "        \n",
    "        # Convert the launch_dict into a Pandas DataFrame\n",
    "        df = pd.DataFrame(launch_dict)\n",
    "        \n",
    "        # Display the DataFrame\n",
    "        print(\"Data extracted into the DataFrame:\")\n",
    "        print(df.head())  # Display the first 5 rows of the DataFrame\n",
    "    else:\n",
    "        print(\"Less than 3 tables found on the page.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.DataFrame({ key: pd.Series(value) for key, value in launch_dict.items() })\n",
    "After you have fill in the parsed launch record values into `launch_dict`, you can create a dataframe from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now export it to a <b>CSV</b> for the next section, but to make the answers consistent and in case you have difficulties finishing this lab. \n",
    "\n",
    "Following labs will be using a provided dataset to make each lab independent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n",
      "Column names found in the third table:\n",
      "['Flight No.', 'Date andtime (UTC)', 'Version,Booster [b]', 'Launch site', 'Payload[c]', 'Payload mass', 'Orbit', 'Customer', 'Launchoutcome', 'Boosterlanding', '1', '2', '3', '4', '5', '6', '7']\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 5 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Skipping row due to mismatch: 9 cells found, expected 17\n",
      "Skipping row due to mismatch: 1 cells found, expected 17\n",
      "Data extracted into the DataFrame with specified dtype:\n",
      "Flight No.               int64\n",
      "Date andtime (UTC)      object\n",
      "Version,Booster [b]     object\n",
      "Launch site             object\n",
      "Payload[c]              object\n",
      "Payload mass            object\n",
      "Orbit                   object\n",
      "Customer                object\n",
      "Launchoutcome           object\n",
      "Boosterlanding          object\n",
      "1                      float64\n",
      "2                      float64\n",
      "3                      float64\n",
      "4                      float64\n",
      "5                      float64\n",
      "6                      float64\n",
      "7                      float64\n",
      "dtype: object\n",
      "Empty DataFrame\n",
      "Columns: [Flight No., Date andtime (UTC), Version,Booster [b], Launch site, Payload[c], Payload mass, Orbit, Customer, Launchoutcome, Boosterlanding, 1, 2, 3, 4, 5, 6, 7]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Static URL for the Falcon 9 and Falcon Heavy launches Wiki page\n",
    "static_url = \"https://en.wikipedia.org/w/index.php?title=List_of_Falcon_9_and_Falcon_Heavy_launches&oldid=1027686922\"\n",
    "\n",
    "# Function to extract column names from <th> elements\n",
    "def extract_column_from_header(header_element):\n",
    "    # Extract the column name text and strip any extra spaces\n",
    "    return header_element.text.strip()\n",
    "\n",
    "# Perform the GET request\n",
    "response = requests.get(static_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Request successful!\")\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    # Ensure there are at least three tables\n",
    "    if len(tables) >= 3:\n",
    "        # Extract the third table (index 2)\n",
    "        third_table = tables[2]\n",
    "        \n",
    "        # Find all <th> elements in the third table (table headers)\n",
    "        headers = third_table.find_all('th')\n",
    "        \n",
    "        # Extract column names from each header element\n",
    "        column_names = [extract_column_from_header(header) for header in headers]\n",
    "        print(\"Column names found in the third table:\")\n",
    "        print(column_names)\n",
    "        \n",
    "        # Initialize the launch_dict with empty lists for each column\n",
    "        launch_dict = {col: [] for col in column_names}\n",
    "        \n",
    "        # Find all rows in the table (skip the first one as it contains headers)\n",
    "        rows = third_table.find_all('tr')[1:]  # Skipping the header row\n",
    "        \n",
    "        # Loop through each row and extract cell data\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            \n",
    "            # Check if the number of cells matches the number of headers\n",
    "            if len(cells) == len(column_names):\n",
    "                for i, cell in enumerate(cells):\n",
    "                    launch_dict[column_names[i]].append(cell.text.strip())  # Append cell data to the corresponding column\n",
    "            else:\n",
    "                print(f\"Skipping row due to mismatch: {len(cells)} cells found, expected {len(column_names)}\")\n",
    "        \n",
    "        # Specify dtype for columns where empty lists might exist\n",
    "        dtype_dict = {\n",
    "            'Flight No.': 'int64',  # Assuming Flight No. should be integer\n",
    "            'Date andtime (UTC)': 'str',\n",
    "            'Version,Booster [b]': 'str',\n",
    "            'Launch site': 'str',\n",
    "            'Payload[c]': 'str',\n",
    "            'Payload mass': 'str',  # Payload mass can be a string if there are units like kg\n",
    "            'Orbit': 'str',\n",
    "            'Customer': 'str',\n",
    "            'Launchoutcome': 'str',\n",
    "            'Boosterlanding': 'str',\n",
    "            # Add dtypes for columns '1' through '7' or others if needed\n",
    "        }\n",
    "        \n",
    "        # Convert the launch_dict into a Pandas DataFrame, with dtype specified for empty lists\n",
    "        df = pd.DataFrame(launch_dict).astype(dtype_dict, errors='ignore')\n",
    "        \n",
    "        # Display the DataFrame\n",
    "        print(\"Data extracted into the DataFrame with specified dtype:\")\n",
    "        print(df.dtypes)  # Display the data types of each column\n",
    "        print(df.head())  # Display the first 5 rows of the DataFrame\n",
    "        \n",
    "    else:\n",
    "        print(\"Less than 3 tables found on the page.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year in the first row of 'static_fire_date_utc' is: 2006\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Perform an HTTP GET request to the SpaceX API (for example, the launches endpoint)\n",
    "spacex_api_url = \"https://api.spacexdata.com/v4/launches\"\n",
    "response = requests.get(spacex_api_url)\n",
    "\n",
    "# Step 2: Convert the response to JSON and normalize it into a DataFrame\n",
    "launch_data = response.json()\n",
    "df = pd.json_normalize(launch_data)\n",
    "\n",
    "# Step 3: Extract the first row from the 'static_fire_date_utc' column and get the year\n",
    "first_static_fire_date = df['static_fire_date_utc'].iloc[0]\n",
    "year = first_static_fire_date[:4]  # Extract the first four characters (year)\n",
    "print(f\"The year in the first row of 'static_fire_date_utc' is: {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rocket DataFrame columns:\n",
      "Index(['payload_weights', 'flickr_images', 'name', 'type', 'active', 'stages',\n",
      "       'boosters', 'cost_per_launch', 'success_rate_pct', 'first_flight',\n",
      "       'country', 'company', 'wikipedia', 'description', 'id', 'height.meters',\n",
      "       'height.feet', 'diameter.meters', 'diameter.feet', 'mass.kg', 'mass.lb',\n",
      "       'first_stage.thrust_sea_level.kN', 'first_stage.thrust_sea_level.lbf',\n",
      "       'first_stage.thrust_vacuum.kN', 'first_stage.thrust_vacuum.lbf',\n",
      "       'first_stage.reusable', 'first_stage.engines',\n",
      "       'first_stage.fuel_amount_tons', 'first_stage.burn_time_sec',\n",
      "       'second_stage.thrust.kN', 'second_stage.thrust.lbf',\n",
      "       'second_stage.payloads.composite_fairing.height.meters',\n",
      "       'second_stage.payloads.composite_fairing.height.feet',\n",
      "       'second_stage.payloads.composite_fairing.diameter.meters',\n",
      "       'second_stage.payloads.composite_fairing.diameter.feet',\n",
      "       'second_stage.payloads.option_1', 'second_stage.reusable',\n",
      "       'second_stage.engines', 'second_stage.fuel_amount_tons',\n",
      "       'second_stage.burn_time_sec', 'engines.isp.sea_level',\n",
      "       'engines.isp.vacuum', 'engines.thrust_sea_level.kN',\n",
      "       'engines.thrust_sea_level.lbf', 'engines.thrust_vacuum.kN',\n",
      "       'engines.thrust_vacuum.lbf', 'engines.number', 'engines.type',\n",
      "       'engines.version', 'engines.layout', 'engines.engine_loss_max',\n",
      "       'engines.propellant_1', 'engines.propellant_2',\n",
      "       'engines.thrust_to_weight', 'landing_legs.number',\n",
      "       'landing_legs.material'],\n",
      "      dtype='object')\n",
      "Merged DataFrame columns:\n",
      "Index(['static_fire_date_utc', 'static_fire_date_unix', 'net', 'window',\n",
      "       'rocket', 'success', 'failures', 'details', 'crew', 'ships', 'capsules',\n",
      "       'payloads', 'launchpad', 'flight_number', 'name_x', 'date_utc',\n",
      "       'date_unix', 'date_local', 'date_precision', 'upcoming', 'cores',\n",
      "       'auto_update', 'tbd', 'launch_library_id', 'id_x', 'fairings.reused',\n",
      "       'fairings.recovery_attempt', 'fairings.recovered', 'fairings.ships',\n",
      "       'links.patch.small', 'links.patch.large', 'links.reddit.campaign',\n",
      "       'links.reddit.launch', 'links.reddit.media', 'links.reddit.recovery',\n",
      "       'links.flickr.small', 'links.flickr.original', 'links.presskit',\n",
      "       'links.webcast', 'links.youtube_id', 'links.article', 'links.wikipedia',\n",
      "       'fairings', 'id_y', 'name_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Perform an HTTP GET request to the SpaceX API (for launches)\n",
    "spacex_api_url = \"https://api.spacexdata.com/v4/launches\"\n",
    "response = requests.get(spacex_api_url)\n",
    "\n",
    "# Step 2: Convert the response to JSON and normalize it into a DataFrame\n",
    "launch_data = response.json()\n",
    "launch_df = pd.json_normalize(launch_data)\n",
    "\n",
    "# Step 3: Perform an HTTP GET request to get rocket details\n",
    "rocket_api_url = \"https://api.spacexdata.com/v4/rockets\"\n",
    "rocket_response = requests.get(rocket_api_url)\n",
    "rocket_data = rocket_response.json()\n",
    "rocket_df = pd.json_normalize(rocket_data)\n",
    "\n",
    "# Step 4: Print the columns of the rocket DataFrame to identify the column that contains the rocket name\n",
    "print(\"Rocket DataFrame columns:\")\n",
    "print(rocket_df.columns)\n",
    "\n",
    "# Step 5: Merge the launch data with the rocket data using the correct ID from the 'rocket' column\n",
    "merged_df = launch_df.merge(rocket_df[['id', 'name']], left_on='rocket', right_on='id', how='left')\n",
    "\n",
    "# Step 6: Print the columns of the merged DataFrame to ensure the merge worked correctly\n",
    "print(\"Merged DataFrame columns:\")\n",
    "print(merged_df.columns)\n",
    "\n",
    "# Step 7: Now filter the DataFrame to keep only Falcon 9 launches based on the correct column for the rocket name\n",
    "# Verify that the 'name' column exists in the merged_df\n",
    "if 'name' in merged_df.columns:\n",
    "    falcon_9_df = merged_df[merged_df['name'] == 'Falcon 9']\n",
    "    # Step 8: Count the number of Falcon 9 launches\n",
    "    falcon_9_count = len(falcon_9_df)\n",
    "    print(f\"The number of Falcon 9 launches is: {falcon_9_count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['static_fire_date_utc', 'static_fire_date_unix', 'net', 'window',\n",
      "       'rocket', 'success', 'failures', 'details', 'crew', 'ships', 'capsules',\n",
      "       'payloads', 'launchpad', 'flight_number', 'name', 'date_utc',\n",
      "       'date_unix', 'date_local', 'date_precision', 'upcoming', 'cores',\n",
      "       'auto_update', 'tbd', 'launch_library_id', 'id', 'fairings.reused',\n",
      "       'fairings.recovery_attempt', 'fairings.recovered', 'fairings.ships',\n",
      "       'links.patch.small', 'links.patch.large', 'links.reddit.campaign',\n",
      "       'links.reddit.launch', 'links.reddit.media', 'links.reddit.recovery',\n",
      "       'links.flickr.small', 'links.flickr.original', 'links.presskit',\n",
      "       'links.webcast', 'links.youtube_id', 'links.article', 'links.wikipedia',\n",
      "       'fairings'],\n",
      "      dtype='object')\n",
      "Possible columns related to pads: ['launchpad']\n",
      "The number of missing values in the 'launchpad' column is: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Perform an HTTP GET request to the SpaceX API (for launches)\n",
    "spacex_api_url = \"https://api.spacexdata.com/v4/launches\"\n",
    "response = requests.get(spacex_api_url)\n",
    "\n",
    "# Step 2: Convert the response to JSON and normalize it into a DataFrame\n",
    "launch_data = response.json()\n",
    "df = pd.json_normalize(launch_data)\n",
    "\n",
    "# Step 3: Check the columns in the DataFrame to find the correct column name\n",
    "print(df.columns)\n",
    "\n",
    "# Step 4: If 'landingPad' is present with a different name, adjust it. Assuming the column might be 'launchpad'\n",
    "# For now, let's count missing values in all columns related to landing/launch pads\n",
    "possible_columns = [col for col in df.columns if 'pad' in col.lower()]\n",
    "print(f\"Possible columns related to pads: {possible_columns}\")\n",
    "\n",
    "# If 'launchpad' is identified, count missing values (you can replace 'launchpad' with the correct column name)\n",
    "if 'launchpad' in df.columns:\n",
    "    missing_values_count = df['launchpad'].isnull().sum()\n",
    "    print(f\"The number of missing values in the 'launchpad' column is: {missing_values_count}\")\n",
    "else:\n",
    "    print(\"The 'landingPad' or similar column does not exist in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['static_fire_date_utc', 'static_fire_date_unix', 'net', 'window',\n",
      "       'rocket', 'success', 'failures', 'details', 'crew', 'ships', 'capsules',\n",
      "       'payloads', 'launchpad', 'flight_number', 'name', 'date_utc',\n",
      "       'date_unix', 'date_local', 'date_precision', 'upcoming', 'cores',\n",
      "       'auto_update', 'tbd', 'launch_library_id', 'id', 'fairings.reused',\n",
      "       'fairings.recovery_attempt', 'fairings.recovered', 'fairings.ships',\n",
      "       'links.patch.small', 'links.patch.large', 'links.reddit.campaign',\n",
      "       'links.reddit.launch', 'links.reddit.media', 'links.reddit.recovery',\n",
      "       'links.flickr.small', 'links.flickr.original', 'links.presskit',\n",
      "       'links.webcast', 'links.youtube_id', 'links.article', 'links.wikipedia',\n",
      "       'fairings'],\n",
      "      dtype='object')\n",
      "The 'landing_pad' column does not exist in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Perform an HTTP GET request to the SpaceX API (for launches)\n",
    "spacex_api_url = \"https://api.spacexdata.com/v4/launches\"\n",
    "response = requests.get(spacex_api_url)\n",
    "\n",
    "# Step 2: Convert the response to JSON and normalize it into a DataFrame\n",
    "launch_data = response.json()\n",
    "df = pd.json_normalize(launch_data)\n",
    "\n",
    "# Step 3: Check the available columns in the DataFrame to identify the correct column related to 'landingPad'\n",
    "print(df.columns)\n",
    "\n",
    "# Step 4: Assuming the column might be 'landing_pad' or something similar, let's count the missing values\n",
    "if 'landing_pad' in df.columns:\n",
    "    missing_values_count = df['landing_pad'].isnull().sum()\n",
    "    print(f\"The number of missing values in the 'landing_pad' column is: {missing_values_count}\")\n",
    "else:\n",
    "    print(\"The 'landing_pad' column does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in the 'launchpad' column is: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Perform an HTTP GET request to the SpaceX API (for launches)\n",
    "spacex_api_url = \"https://api.spacexdata.com/v4/launches\"\n",
    "response = requests.get(spacex_api_url)\n",
    "\n",
    "# Step 2: Convert the response to JSON and normalize it into a DataFrame\n",
    "launch_data = response.json()\n",
    "df = pd.json_normalize(launch_data)\n",
    "\n",
    "# Step 3: Check for missing values in the 'launchpad' column\n",
    "missing_values_count = df['launchpad'].isnull().sum()\n",
    "\n",
    "# Step 4: Output the number of missing values in the 'launchpad' column\n",
    "print(f\"The number of missing values in the 'launchpad' column is: {missing_values_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in the 'launchpad' column is: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the 'launchpad' column, including empty strings and placeholders\n",
    "missing_values_count = df['launchpad'].apply(lambda x: x is None or x == \"\" or x == \"N/A\").sum()\n",
    "\n",
    "# Output the number of missing values in the 'launchpad' column\n",
    "print(f\"The number of missing values in the 'launchpad' column is: {missing_values_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now export it to a <b>CSV</b> for the next section, but to make the answers consistent and in case you have difficulties finishing this lab. \n",
    "\n",
    "Following labs will be using a provided dataset to make each lab independent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in the 'launchpad' column is: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the 'launchpad' column, including more placeholders\n",
    "missing_values_count = df['launchpad'].apply(lambda x: x is None or x == \"\" or x == \"N/A\" or x == \"unknown\" or x == \"None\").sum()\n",
    "\n",
    "# Output the number of missing values in the 'launchpad' column\n",
    "print(f\"The number of missing values in the 'launchpad' column is: {missing_values_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['static_fire_date_utc', 'static_fire_date_unix', 'net', 'window',\n",
      "       'rocket', 'success', 'failures', 'details', 'crew', 'ships', 'capsules',\n",
      "       'payloads', 'launchpad', 'flight_number', 'name', 'date_utc',\n",
      "       'date_unix', 'date_local', 'date_precision', 'upcoming', 'cores',\n",
      "       'auto_update', 'tbd', 'launch_library_id', 'id', 'fairings.reused',\n",
      "       'fairings.recovery_attempt', 'fairings.recovered', 'fairings.ships',\n",
      "       'links.patch.small', 'links.patch.large', 'links.reddit.campaign',\n",
      "       'links.reddit.launch', 'links.reddit.media', 'links.reddit.recovery',\n",
      "       'links.flickr.small', 'links.flickr.original', 'links.presskit',\n",
      "       'links.webcast', 'links.youtube_id', 'links.article', 'links.wikipedia',\n",
      "       'fairings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the exact column names to confirm the correct name of the 'launchpad' column\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5e9e4502f5090995de566f86' '5e9e4501f509094ba4566f84'\n",
      " '5e9e4502f509092b78566f87' '5e9e4502f509094188566f88']\n"
     ]
    }
   ],
   "source": [
    "# Output all unique values in the 'launchpad' column\n",
    "unique_values = df['launchpad'].unique()\n",
    "print(unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>df.to_csv('spacex_web_scraped.csv', index=False)</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in DataFrame:\n",
      "Index(['static_fire_date_utc', 'static_fire_date_unix', 'net', 'window',\n",
      "       'rocket', 'success', 'failures', 'details', 'crew', 'ships', 'capsules',\n",
      "       'payloads', 'launchpad', 'flight_number', 'name', 'date_utc',\n",
      "       'date_unix', 'date_local', 'date_precision', 'upcoming', 'cores',\n",
      "       'auto_update', 'tbd', 'launch_library_id', 'id', 'fairings.reused',\n",
      "       'fairings.recovery_attempt', 'fairings.recovered', 'fairings.ships',\n",
      "       'links.patch.small', 'links.patch.large', 'links.reddit.campaign',\n",
      "       'links.reddit.launch', 'links.reddit.media', 'links.reddit.recovery',\n",
      "       'links.flickr.small', 'links.flickr.original', 'links.presskit',\n",
      "       'links.webcast', 'links.youtube_id', 'links.article', 'links.wikipedia',\n",
      "       'fairings'],\n",
      "      dtype='object')\n",
      "Number of missing values in 'launchpad': 0\n",
      "Number of placeholder values in 'launchpad': 0\n",
      "Data type of 'launchpad': object\n",
      "Unique values in 'launchpad' column:\n",
      "['5e9e4502f5090995de566f86' '5e9e4501f509094ba4566f84'\n",
      " '5e9e4502f509092b78566f87' '5e9e4502f509094188566f88']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Perform an HTTP GET request to the SpaceX API (for launches)\n",
    "spacex_api_url = \"https://api.spacexdata.com/v4/launches\"\n",
    "response = requests.get(spacex_api_url)\n",
    "\n",
    "# Step 2: Convert the response to JSON and normalize it into a DataFrame\n",
    "launch_data = response.json()\n",
    "df = pd.json_normalize(launch_data)\n",
    "\n",
    "# Step 3: Check the available columns to ensure 'launchpad' exists\n",
    "print(\"Available columns in DataFrame:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Step 4: Check for missing values in the 'launchpad' column\n",
    "if 'launchpad' in df.columns:\n",
    "    missing_values_count = df['launchpad'].isnull().sum()\n",
    "    print(f\"Number of missing values in 'launchpad': {missing_values_count}\")\n",
    "\n",
    "    # Step 5: Check for common placeholders for missing values ('N/A', 'unknown', 'None', etc.)\n",
    "    placeholders = ['N/A', 'None', 'unknown', '']\n",
    "    placeholder_count = df['launchpad'].isin(placeholders).sum()\n",
    "    print(f\"Number of placeholder values in 'launchpad': {placeholder_count}\")\n",
    "\n",
    "    # Step 6: Check the data type of the 'launchpad' column\n",
    "    print(f\"Data type of 'launchpad': {df['launchpad'].dtype}\")\n",
    "\n",
    "    # Step 7: Output unique values in the 'launchpad' column\n",
    "    print(\"Unique values in 'launchpad' column:\")\n",
    "    print(df['launchpad'].unique())\n",
    "else:\n",
    "    print(\"The 'launchpad' column does not exist in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.linkedin.com/in/yan-luo-96288783/\">Yan Luo</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.linkedin.com/in/nayefaboutayoun/\">Nayef Abou Tayoun</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description      |\n",
    "| ----------------- | ------- | ---------- | ----------------------- |\n",
    "| 2021-06-09        | 1.0     | Yan Luo    | Tasks updates           |\n",
    "| 2020-11-10        | 1.0     | Nayef      | Created the initial version |\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright  2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "64f1b0aac408997185c47caba18730e0028b75e7934a0e5bf0ae73c5cb7ba677"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
